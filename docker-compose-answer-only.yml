services:
  llm-server:
    build:
      context: .
      #dockerfile: Dockerfile.server # original one, below is for testing
      dockerfile: ATE/test_compare/Dockerfile.server.comparison
    container_name: llm-server
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              #count: 1
              capabilities: [gpu]
              device_ids: ['1']
    environment:
      - HF_HOME=/huggingface_cache
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    volumes:
      - /home/belletti/huggingface_cache:/huggingface_cache
      - /home/belletti/ATE-metric-calling/ATE:/home/belletti/ATE-metric-calling/ATE
      - /home/belletti/ATE-metric-calling/finetuning/outputs:/home/belletti/ATE-metric-calling/finetuning/outputs
    ports:
      - "8000:8000"

  llm-main:
    build:
      context: .
      dockerfile: Dockerfile.main
    container_name: llm-main
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              #count: 1
              capabilities: [gpu]
              device_ids: ['1']
    environment:
      - HF_HOME=/huggingface_cache
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - NUM_EPISODES=40
      - NUM_STM_SLOTS=4
      - MAX_TURN=4
      - MODEL_SERVER_URL=http://llm-server:8000/generate
      - MODEL_NAME=/home/belletti/ATE-metric-calling/finetuning/outputs/metric_evaluation_assistant/final_model
    volumes:
      - /home/belletti/ATE-metric-calling/ATE:/home/belletti/ATE-metric-calling/ATE
      - ./ATE/results:/home/belletti/ATE-metric-calling/ATE/results
      - /home/belletti/huggingface_cache:/huggingface_cache
    depends_on:
      - llm-server
    
  model-comparison:
    build:
      context: .
      dockerfile: Dockerfile.main
    volumes:
      - huggingface_cache:/huggingface_cache
      - ./ATE/test_compare/outputs:/home/belletti/ATE-metric-calling/ATE/test_compare/outputs
      - ./ATE:/home/belletti/ATE-metric-calling/ATE
      - /home/belletti/ATE-metric-calling/finetuning/outputs:/home/belletti/ATE-metric-calling/finetuning/outputs
    environment:
      - MODEL_SERVER_URL=http://0.0.0.0:8000/generate
      - PYTHONPATH=/home/belletti/ATE-metric-calling
    depends_on:
      - llm-server
    command: compare

volumes:
  huggingface_cache: